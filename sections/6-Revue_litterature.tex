\Chapter{REVUE DE LITTÉRATURE / LITERATURE REVIEW}\label{sec:RevLitt}

\section{Trace}

\subsection{Analyse de trace}

Le traçage implique une utilisation spécialisée de la journalisation qui enregistre les événements d'exécution concernant l'exécution d'un programme. Le traçage est utilisé pour détecter et identifier les problèmes de performances, les problèmes de comportement et les problèmes de sécurité (TROUVEZ LA SOURCE ICI). Le traçage peut être effectué à plusieurs niveaux, notamment l'espace utilisateur, le noyau, le matériel, l'hyperviseur, le réseau, etc. Le logiciel qui capture ces événements est appelé traceur. Pour capturer ces événements, les points de trace doivent être insérés de manière statique ou dynamique. Dans le cas où le point de trace est inséré de manière statique, le code doit être modifié pour inclure les macros de traçage et doit être recompilé. Dans le cas où le point de trace est inséré dynamiquement, les points de trace sont ajoutés dynamiquement à un programme compilé et en cours d'exécution \cite{gregg2011dtrace}. Lors de l'exécution du programme, des événements seront émis et un traceur les capturera. À ce stade, un traceur produira un fichier de trace, organisé dans un format spécifique. Étant donné que le traceur doit s'exécuter en même temps que l'application principale, cela aura un impact sur le temps d'exécution de l'application principale. C'est pourquoi un traceur doit avoir une surcharge d'exécution minimale. Lorsque le traceur a fini de produire un fichier de trace, nous pouvons appliquer une analyse de trace.

Linux inclut perf \cite{kernelPerfWiki}, un ensemble d'outils qui utilisent l'infrastructure perf\_event\_open \cite{man7Perf}. Ces outils puissants couvrent un large éventail d'informations liées aux performances disponibles pour le système d'exploitation. Les fonctionnalités disponibles incluent l'échantillonnage basé sur un compteur de performances, certains points d'instrumentation, la journalisation pour enregistrer les informations collectées et la visualisation sous forme de profils textuels. Cependant, perf utilise un format de fichier monolithique et ne dispose pas d'un moyen évolutif et convivial pour analyser et visualiser les délais résultants. Auparavant, nous utilisions perf pour la surveillance des performances et convertissions la trace résultante dans un format de fichier différent permettant l'utilisation d'outils de visualisation de chronologie \cite{schone2014scalable}.

OProfile \cite{sourceforgeAboutOProfile} est un profileur statistique non intrusif construit sur l'infrastructure \textit{perf\_event-\_open}. Il prend en charge la surveillance à faible coût de l'ensemble du système et des processus uniques, mais est limité aux profils agrégés uniquement.

Une analyse de trace transforme les événements de trace en un ensemble de données pour effectuer plus d'investigation et les réorganise en structure de données pour un accès plus rapide \cite{prieur2018r}.

HPCToolkit \cite{adhianto2010hpctoolkit} est une suite d'outils d'analyse des performances qui se concentre sur l'échantillonnage d'applications parallèles, prenant en charge MPI et OpenMP. Il utilise la synthèse pour fournir des profils de performances. De plus, une vue chronologique basée sur la journalisation est disponible. HPCToolkit comprend des informations provenant d'échantillons de pile d'appels qui sont traités dans l'espace utilisateur avec des techniques de déroulement sophistiquées et des compteurs de performances de l'interface de programmation d'applications de performances.

Trace Viewer est une extension open source disponible dans l'IDE Theia. Il utilise le protocole Trace-Server-Protocol pour interroger les modèles de données et visualiser différentes analyses de trace. Une analyse de trace lit une trace de manière séquentielle et stocke ses résultats (appelés modèles d'état) dans une structure de données pouvant être écrite sur disque. À partir des événements de trace, l'analyse extrairait les intervalles des valeurs d'état pour différentes ressources système (CPU, processus, disques, etc.). \cite{chen2021distributed}



\subsection{Outils de visualisation de trace}

Efftinge et Kosyakov ont présenté Theia \cite{theia_2017}, un nouveau logiciel libre cadriciel pour les EDI. Theia a été mis de l'avant pour montrer la prochaine génération d'EDI après Eclipse. L'architecture de Theia a été penser pour être extensible, supporter plusieurs languages, supporter le lancement et le déboguage d'application. L'avantage principal de Theia est qui pourra être exécuter dans les navigateurs web et en local comme une application de bureau directement sur le système d'exploitation. Theia est un cadriciel d'EDI ce qui signifie qu'il est sera utilisé pour créer d'autre EDI qui auront toutes les avantages que Theia offre, mais personnaliser pour les besoins de l'utilisateur.


Le projet partage de nombreuses similitudes avec l'éditeur de code Microsoft, Visual Studio Code au niveau des technologies, de l'interface graphique et les personnalisations de l'EDI par plugins. Theia utilise Node.js, Electron comme technologie et implémente les protocoles LSP et DAP. Theia permet également d'installer des extensions lors de la compilation pour qu'elle fasse partie intégrante de l'EDI lors de la distribution. Les extensions entre Theia et VsCode sont compatibles.

% Theia
L'extension Theia Trace Viewer est la nouvelle application serveur-client développée par Ericsson et la Fondation Eclipse, avec d'autres contributeurs. Son objectif est de remplacer Trace Compass en tant qu'outil de traçage à usage général d'Eclipse. Comme décrit précédemment, Trace Viewer utilise une architecture serveur-client, dans laquelle le client communique avec le serveur via un protocole appelé Trace Server Protocol (TSP). \cite{desnoyers2006lttng} L'extension s'intègre dans l'EDI permettant de programmer, de tracer et de visualiser avec un outil et permet d'itérer plus rapidement dans les différentes phases de développement. L'extension permet de visualiser les traces sous plusieurs vues selon les analyses disponibles pour la trace dans le serveur.


% Grafana
Grafana est une plateforme libre logiciel qui fournit des fonctionnalités pour interroger, visualiser et comprendre les mesures. Grafana a répondu aux questions sur la façon de visualiser les données, ce qui a mis au défi de nombreux développeurs ces dernières années. Grafana apporte toutes les données ensemble et les partage entre les équipes. Grâce à divers plugins, les utilisateurs peuvent obtenir ce qui leur convient le plus, ou même créer des plugins adaptés à leurs besoins. Grafana offre plusieurs fonctionnalités pour permettre à l'utilisateur de visualiser les données, d'organiser les données ou même de notifier sur différente plateforme tel que slack ou PagerDuty. Grafana peut également se connecter à différente source de donnée externe tels que Excel, une base de donnée SQL et immédiatement permettre de visualiser ses données. Cela est très pratique puisque Grafana interprête directement les données au lieu de le faire manuellement ce qui nous sauve une étape qui peut parfois prendre du temps. Il permet également d'afficher plusieurs source de données différentes dans la même vue. Grafana est assez simple lorsqu'il est temps d'afficher les données. Après s'être connecté au différente source de donnée, l'utilisateur doit créer construire une requête pour extraire l'information pertinent. Ensuite, pour le cas d'utilisation l'utilisateur choisir l'outil de visualisation. \cite{do2021data} Grafana ne s'occupe par n'en plus d'instrumenter les applications, mais laisse plûtot le travail à d'autre outil plus spécialisé pour ce traitement comme Prometheus. À prendre en compte que Grafana n'offre pas des analyses sur un ensemble de donnée comme Trace Compass offre, mais réellement la possibilité d'afficher différentes données selon la requête construite.


% Vampir
Vampir fournit un cadre facile à utiliser qui permet aux développeurs d'afficher et d'analyser rapidement le comportement arbitraire d'un programme. La suite d'outils implémente des algorithmes d'analyse d'événements optimisés et des affichages personnalisables \cite{GmbH_Vampir}. L'optimisation des performances avec Vampir implique plusieurs étapes pour améliorer les performances des applications informatiques parallèles et hautes performances. Vampir est un outil d'analyse des performances souvent utilisé en conjonction avec l'instrumentation Score-P pour collecter des données sur les performances des applications \cite{Mix_2018}. Le processus commence généralement par l'instrumentation de votre code et son exécution, en collectant des données. Vampir est ensuite utilisé pour visualiser et analyser ces données. L'analyse se concentre sur l'identification des goulots d'étranglement, tels que les algorithmes inefficaces, les modèles d'accès aux données problématiques, les charges de travail déséquilibrées et les zones à forte utilisation du processeur. Les modèles de communication et de synchronisation sont essentiels, et Vampir aide à évaluer comment les processus communiquent et où se produit la surcharge de synchronisation. Les modèles d'utilisation de la mémoire sont également examinés, car une consommation élevée de mémoire et un accès inefficace à la mémoire peuvent affecter les performances. Les stratégies de parallélisation sont évaluées et des modifications sont apportées en fonction des informations tirées de l'analyse Vampir. Ces changements peuvent impliquer des modifications d'algorithmes, une optimisation des structures de données, une réduction des transferts de données et un meilleur équilibrage de charge. Une analyse comparative continue est cruciale pour mesurer l'impact des efforts d'optimisation. Pour les systèmes distribués, Vampir peut être utilisé pour le profilage parallèle, vous permettant d'analyser les performances sur plusieurs noeuds. La collaboration et la documentation des résultats sont essentielles, car des perspectives multiples et des efforts collectifs conduisent souvent à des solutions d'optimisation plus efficaces. En résumé, Vampir est un outil précieux dans le processus itératif d'optimisation des performances, qui implique un profilage, une analyse minutieuse et des modifications du code.


\section{Traçage distribué}

\subsection{Système distribué}

Les services Internet modernes sont souvent mis en oeuvre sous la forme de systèmes distribués complexes à grande échelle. Ces applications sont construites à partir d'ensembles de modules logiciels qui peuvent être développés par différentes équipes, peut-être dans différents langages de programmation, et peuvent s'étendre sur plusieurs milliers de machines réparties dans plusieurs installations physiques. Les outils qui aident à comprendre le comportement du système et à raisonner sur les problèmes de performances sont inestimables dans un tel environnement.

L'intelligence artificielle (IA) et le cloud computing sont apparus comme une voie prometteuse pour répondre aux demandes informatiques croissantes des applications d'IA. La formation parallèle et distribuée des modèles d'IA devient de plus en plus complexe. Les techniques de formation parallèles et distribuées sont apparues comme des approches essentielles pour réduire le temps de formation et améliorer l'utilisation des ressources. En tirant parti de ces techniques, les chercheurs et les praticiens peuvent accélérer le processus de formation, améliorer les performances des modèles et réduire les coûts associés aux systèmes d'IA basés sur le cloud \cite{mungoli2023scalable}.

Les demandes en ressources des applications HPC varient considérablement. Les demandes variables en ressources peuvent conduire à ce que les ressources HPC ne soient pas pleinement utilisées. Il devient très difficile pour les systèmes HPC de trouver les raisons de la limitation des tâches. \cite{li2023analyzing}

\subsection{Outils de traçage distribué}

Les efforts récents pour paralléliser l'analyse des traces, comme en témoignent les travaux de Reumont-Locke \cite{reumont2015methodes} et Martin \cite{Martin2018}, ont visé à améliorer l'efficacité de ce processus. Néanmoins, l'optimisation de l'efficacité ne peut à elle seule répondre pleinement aux défis d'évolutivité inhérents aux environnements distribués. Même si un outil d'analyse de trace fonctionne avec une efficacité maximale, il reste limité par les capacités de calcul du matériel sous-jacent, généralement équivalentes au système tracé dans le meilleur des cas. Lorsque nous augmentons le nombre de nœuds tracés jusqu'à des centaines ou des milliers, l'outil d'analyse devient rapidement submergé. Ce goulot d'étranglement devient particulièrement problématique dans les applications nécessitant une disponibilité continue, car les anomalies peuvent rapidement s'aggraver au sein des clusters \cite{matloff2011programming}, et les retards dans l'analyse des données pendant les enquêtes ne font qu'exacerber la situation.

Une approche similaire observée dans les plateformes de traçage distribuées implique le déploiement de plusieurs traceurs, chacun fonctionnant sur une machine tracée, toutes les données de trace étant transmises à un collecteur central pour une analyse ultérieure. Dans certains cas, ces traceurs prennent la forme de bibliothèques client, permettant la création d'événements personnalisés avec des informations spécifiques à l'application, contrairement à l'approche de traçage plus opaque en boîte noire d'outils comme Nagios. Prometheus répond aux problèmes d'évolutivité en permettant l'organisation des instances Prometheus dans une fédération, comme l'a démontré Reback en 2021 \cite{Logz.io_prometheus_2023}. Cette approche facilite l'établissement d'une hiérarchie à plusieurs niveaux de serveurs Prometheus, chaque niveau regroupant et transmettant les données au niveau supérieur. Notamment, Nagios a également adopté une approche similaire pour atténuer ses propres défis d'évolutivité (Nagios) \cite{Nagios2019}.

Bien que la mise à l'échelle horizontale des serveurs d'analyse soit une stratégie efficace, il est essentiel de reconnaître que les analyses Prometheus consistent principalement en des métriques simples, ce qui rend l'agrégation possible. Cependant, appliquer le même modèle à des processus d'analyse plus complexes, tels que ceux proposés par Trace Compass, présente des défis considérables. La nature de leurs agrégations varie considérablement selon les différents types d'analyse, ce qui rend difficile l'application directe de l'approche Prometheus dans de tels cas.


\subsection{Outils de visualisation de trace distribué}

Les efforts récents pour paralléliser l'analyse du chemin critique, comme en témoignent les travaux de Denis et Dagenais, ont visé à améliorer l'efficacité de ce processus. Les événements de noyau nécessaires à l'exécution de l'analyse sont identifiés. Une architecture distribuée qui ne nécessite aucun transfert de fichiers entre le client et les noeuds tracés et permet la distribution du calcul d'analyse de trace est présentée. Ils explorent également l'algorithme pour résoudre les dépendances distantes \cite{matloff2011programming}.

Bien que les architectures distribuées offrent une solution au problème d'évolutivité, il est difficile d'exploiter pleinement leurs avantages dus à la communication réseau lors de la diffusion vers tous les noeuds. C'est pourquoi restreindre les communications uniquement au nœud essentiel au calcul est crucial \cite{denys2023distributed}.

\section{Architecture distribué}

L'architecture logicielle est une représentation abstraite de la structure et de l'organisation d'un ou plusieurs systèmes informatiques. Un système est formé par un ensemble de composants logiciels, et les interactions entre ces composants sont désignées sous le nom de "connecteurs". En utilisant des diagrammes et des schémas, l'architecture logicielle décrit comment un système doit être conçu pour répondre à des exigences techniques et fonctionnelles, tout en prenant en compte d'autres caractéristiques logicielles telles que la performance, la réutilisabilité, et la flexibilité \cite{Martens_2010}. Il devient de plus en plus crucial de prendre en considération des critères à long terme lors du choix d'une architecture, car une mauvaise architecture peut entraîner des coûts considérables et altérer la qualité du service offert. \cite{Kolny_2023}

% p.46
\subsection{Client-serveur}

Cette architecture est couramment évoquée lorsqu'il est question de systèmes distribués. Elle détient une importance historique considérable et demeure la plus largement adoptée. La figure X illustre une structure simple dans laquelle les processus endossent les rôles de clients ou de serveurs. Dans ce modèle architectural, deux entités se distinguent par leurs responsabilités : d'une part, le client, dont l'objectif est d'accéder à une ressource, et de l'autre, le serveur, chargé de gérer ces ressources \cite{Terra_2023}. Généralement, un serveur désigne une machine ou un processus doté de puissance de calcul suffisante pour répondre à plusieurs requêtes concurrentes de clients. Les clients n'interagissent pas directement entre eux, mais passent par l'intermédiaire du serveur. En particulier, les processus clients interagissent avec des processus serveurs individuels sur des ordinateurs hôtes potentiellement séparés, ce qui leur permet d'accéder aux ressources partagées qu'ils gèrent. Par ailleurs, les clients ne sont pas nécessairement hébergés sur la même machine physique que le serveur, ce qui nécessite la mise en place d'un protocole de communication entre le client et le serveur. Les serveurs peuvent à leur tour jouer le rôle de clients vis-à-vis d'autres serveurs, comme le montre la figure. Par exemple, un serveur web agit en tant que serveur pour un navigateur web, mais devient lui-même client lorsqu'il interagit avec une base de données \cite{Coulouris_2012}.

L'architecture client-serveur, également connue sous le nom de modèle client-serveur, est un concept de réseau qui répartit les tâches et les charges de travail entre les clients et les serveurs, qu'ils résident sur le même système ou soient connectés via un réseau informatique. Cette architecture se subdivise en plusieurs sous-catégories, notamment le client lourd et le client léger. Dans le premier cas, le client assume une grande part de la responsabilité en ce qui concerne les calculs et le traitement des données, ce qui réduit la tâche du serveur à la simple transmission des informations demandées. Dans le second cas, c'est l'inverse : le serveur prend en charge les calculs et le traitement des données, tandis que le client se charge principalement de l'affichage. Ces deux variantes servent à répondre au besoin de l'application et chacune mitige certains risques des deux côtés.

La communication dans une architecture client-serveur repose largement sur le modèle de protocole de requête-réponse. En réalité, ce protocole est une structure imposée sur un service sous-jacent de transmission de messages, conçue pour faciliter la communication entre les composants du système client-serveur. Ces protocoles impliquent généralement un échange bidirectionnel de messages, allant du client vers le serveur, puis du serveur vers le client. Le premier message contient à la fois une description de l'opération à exécuter sur le serveur et un tableau d'octets contenant les arguments correspondants. Le second message contient les résultats de l'opération, également codés sous forme de tableau d'octets \cite{Coulouris_2012}. Ce paradigme est relativement basique et trouve son utilité principalement dans les systèmes embarqués où les performances sont d'une importance cruciale. De plus, cette approche est également employée dans le protocole HTTP.

\subsection{Pair-à-pair}

Bien que le modèle client-serveur offre une approche directe et relativement simple du partage de données et d'autres ressources, il est peu évolutif. La centralisation de la fourniture et de la gestion des services qu'implique le placement d'un service à une seule adresse ne va pas bien au-delà de la capacité de l'ordinateur qui héberge le service et de la bande passante de ses connexions réseau.

Dans l'architecture pair-à-pair, tous les processus impliqués dans une tâche ou une activité jouent des rôles similaires, interagissant de manière coopérative en tant que pairs, sans aucune distinction entre les processus client et serveur ou les ordinateurs sur lesquels ils s'exécutent. Concrètement, tous les processus participants exécutent le même programme et offrent entre eux le même ensemble d'interfaces. La nécessité de distribuer les ressources partagées beaucoup plus largement afin de partager les charges informatiques et de communication encourues dans y accéder parmi un nombre beaucoup plus grand d'ordinateurs et de liens réseau. L'idée clé qui a conduit au développement des systèmes pair-à-pair est que les ressources réseau et informatiques détenues par les utilisateurs d'un service pourraient également être utilisées pour prendre en charge ce service. Cela a pour conséquence utile que les ressources disponibles pour exécuter le service augmentent avec le nombre d'utilisateurs. La capacité matérielle et les fonctionnalités du système d'exploitation des ordinateurs de bureau d'aujourd'hui dépassent celles des serveurs d'hier, et la majorité d'entre eux sont équipés de connexions réseau haut débit permanentes. Le but de l'architecture pair-à-pair est d'exploiter les ressources (à la fois données et matériel) d'un grand nombre d'ordinateurs participants pour l'accomplissement d'une tâche ou d'une activité donnée. Des applications et des systèmes peer-to-peer ont été construits avec succès pour permettre à des dizaines, voire des centaines de milliers d'ordinateurs, de fournir un accès aux données et à d'autres ressources qu'ils stockent et gèrent collectivement \cite{Coulouris_2012}. L'un des premiers exemples était l'application Napster permettant de partager des fichiers musicaux numériques \cite{Vu2010}. Bien que Napster ne soit pas une pure architecture pair-à-pair, sa démonstration de faisabilité a abouti au développement du modèle architectural dans de nombreuses directions précieuses. Les systèmes de calcul à haute performance, les systèmes de partage de ﬁchiers (BitTorrent) \cite{toole2006bittorrent} et la chaîne de blocs (blockchain) \cite{Abdella_2021} sont des exemples d'applications qui exploitent l'architecture pair-à-pair.

\subsection{Maître-Ouvrier}

L'architecture Maître-Ouvrier est une sorte d'architecture de système de calcul parallèle typique, dans laquelle existe un noeud maître. Le noeud maître est responsable de la segmentation de l'ensemble de la tâche, de la distribution des différentes sous-tâches après segmentation à chaque noeud ouvrier pour le calcul et de la synthèse de tous les résultats de calcul des noeuds ouvrier pour obtenir le résultat final du calcul. Le dessin de l'architecture du système est présenté à la figure X. \cite{LU2020106497}

Dans une telle architecture, chaque noeud ouvrier doit procéder au travail de calcul intense, tandis que le noeud maître s'occupe de coordonner les résultats finaux. Le noeud maître sera principalement responsable des données d'entrée avant le calcul, de la sortie des résultats finaux après le calcul, ainsi que de la segmentation et de la distribution des sous-tâches pendant le processus de calcul. Par conséquent, en utilisant une telle architecture, il sera plus facile de gérer et de contrôler les noeuds de calcul, tels que l'équilibrage de charge. Lorsque la disposition des noeuds maître-ouvrier est adoptée, chaque noeud doit effectuer sa tâche et le calcul associé.

Pour le noeud ouvrier, chaque noeud peut être dans un état différent à différents moments du processus de calcul \cite{Liu_2010}, comme le montre la figure X, les états suivants sont inclus :
\begin{itemize}
    \item INCONNU signifie que l'état du noeud est inconnu, généralement avant l'initialisation du système;
    \item LIBRE signifie que le noeud est inactif et disponible pour recevoir des tâches de calcul afin de calculer;
    \item OCCUPÉ signifie que le noeud a commencé à calculer;
    \item OCCUPÉ\_À\_LIBRE signifie que le noeud a terminé le calcul et attend les données renvoyées. Si les données sont retournées au noeud maître, le statut passera au statut LIBRE;
    \item MORT signifie que le noeud n'est pas disponible, probablement en raison d'une panne de réseau entraînant une absence de réponse du noeud.
\end{itemize}

% p. 206
\section{Interopérabilité de système hétérogène}

L'interopérabilité se réfère à la capacité d'un système à partager des données avec un autre système, même s'ils n'utilisent pas le même langage de programmation. Pour ce faire, les systèmes peuvent interagir de deux manières : soit de manière indirecte en utilisant un référentiel commun, soit directement via un protocole de communication réseau. Dans le premier cas, les systèmes se coordonnent en utilisant des éléments tels qu'une base de données, un système de fichiers distribués ou une zone de mémoire partagée. Dans le second cas, les systèmes communiquent en utilisant des protocoles tels que TCP, UDP ou HTTP et échangent des données dans un format compréhensible mutuellement. Cette section se concentre exclusivement sur les méthodes d'interaction directe entre les systèmes.

\subsection{REST}

Le standard REST est un ensemble de règles et de conventions permettant de créer et d'interagir avec des services Web ou des applications Web. REST est un style architectural pour la conception d'applications en réseau qui se repose sur le protocole de communication HTTP. \cite{RESTChap5}

Le standard REST sont largement utilisées pour créer des serveurs Web en raison de leur simplicité, de leur évolutivité et de leur facilité d'intégration. Ils sont devenus un standard pour les applications Web et sont utilisés dans une large gamme de services, des plateformes de médias sociaux aux sites Web de commerce électronique et aux applications mobiles. Voici quelques principes et concepts clés associés au standard REST: ressources, méthodes HTTP, communication sans état, interface uniforme, représentation \cite{RESTChap5}.

Ressources: Les données et les fonctionnalités sont exposées en tant que ressources. Ces ressources sont identifiées par des URL, et chaque ressource possède une URL unique. Par exemple, un API REST pour une librairie peut avoir des ressources telles que "/livres", "/auteurs" et "/commandes".

Méthodes HTTP : Les méthodes HTTP permet d'effectuer des opérations sur les ressources. Les méthodes HTTP les plus couramment utilisées dans REST sont :
\begin{itemize}
    \item GET Utilisé pour récupérer des données d'une ressource;
    \item POST Utilisé pour créer une nouvelle ressource
    \item PUT Utilisé pour mettre à jour une ressource existante ou la créer si elle n'existe pas;
    \item DELETE Utilisé pour supprimer une ressource;
    \item PATCH Utilisé pour effectuer des mises à jour partielles d'une ressource.
\end{itemize}


Communication sans état: Une communication sans état signifie que chaque requête doit contenir toutes les informations nécessaires pour comprendre et traiter la requête. Le serveur ne stocke aucune information client entre les requêtes. Cette simplicité et cette apatride rendent les API REST évolutives et faciles à mettre en cache.

Interface Uniforme: Une interface uniforme inclut l'utilisation de code d'état HTTP pour les réponses (par exemple, 200 pour succès, 404 pour non trouvé, 500 pour les erreurs du serveur) et l'appellation des URIs doit être claire et cohérente des ressources.

Représentation: Les données sont transférées sous forme de représentations, qui peuvent être dans différents formats, tels que JSON, XML, HTML ou autres. Les clients peuvent demander le format de représentation qu'ils préfèrent en définissant les en-têtes HTTP appropriés.

\subsection{GraphQL}

Fondamentalement, GraphQL offre aux clients la possibilité d'interroger une base de données en utilisant un schéma comme représentation. Cette approche marque une différence majeure par rapport aux standards REST. Dans REST, les serveurs exposent une liste de point d'accès que les clients peuvent appeler, tandis que les serveurs GraphQL mettent à disposition une base de données que les clients peuvent interroger avec un seul point d'accès. Une base de données GraphQL est définie par un schéma, qui peut être considéré comme un réseau de relations \cite{hartig2017initial}. Dans ce réseau, les noeuds représentent des objets qui définissent un type et comprennent une liste de champs. Chaque champ a un nom et un type associé. Des liens se forment lorsque les objets définissent des champs dont les types sont d'autres objets. GraphQL propose un langage de définition de schéma (DSL) simple. Pour illustrer son utilisation, prenons l'exemple d'un système de blog simple comprenant deux types d'objets : Publication et Auteur. Comme illustré dans l'exemple (Listing X), les types d'objets sont définis à l'aide du mot-clé "type". Dans ce cas, le type Publication comporte quatre champs : id, auteur, titre, et texte. Le premier champ est une chaîne non nullable (indiqué par le "!" après le type), le deuxième champ fait référence à un autre type d'objet du schéma, appelé "Auteur". Les deux champs restants dans Publication ont un type "String". Enfin, les schémas incluent généralement un type pré-défini appelé "Query", qui sert de point d'entrée aux API GraphQL. Un objet de type Query expose les types d'objets qui peuvent être interrogés par les clients, ainsi que les arguments nécessaires pour effectuer ces requêtes. Par exemple, la requête "publier" accepte une chaîne non nullable comme argument et renvoie l'objet de type Publication ayant cette chaîne comme identifiant. \cite{Brito2019}

\begin{lstlisting}
type Publication {
    id: String!
    auteur: Auteur
    titre: String
    texte: String
}
type Auteur {
    id: String!
    nom: String
    courriel: String
}
type Query {
    publier(id: String!): Publication
}
\end{lstlisting}

GraphQL définit également un langage de requête utilisé par les clients. Dans l'exemple ci-dessous (Listing X), nous présentons trois types de requêtes dans ce langage. La première requête, nommée "PublierParTitre", permet au client de demander l'objet "Publication" avec l'identifiant 1000, en spécifiant qu'il souhaite uniquement récupérer le champ "Titre" de cet objet. La deuxième requête, intitulée "PublierParTitreEtTexte", suit une logique similaire, mais cette fois-ci, le client demande les champs "titre" et "texte" de l'objet "Publication". Enfin, la troisième requête, appelée "PublierParTitreEtAuteur", cherche à obtenir à la fois le titre et l'auteur de la même "Publication". Cependant, étant donné que l'auteur est un autre objet, il est nécessaire de préciser les champs de l'auteur que le client souhaite interroger, en l'occurrence, uniquement le nom.

\begin{lstlisting}
query PublierParTitre{
    publier(id:"1000"){
        titre
    }
}
query PublierParTitreEtTexte{
    publier(id:"1000"){
        titre
        texte
    }
}
query PublierParTitreEtAuteur{
    publier(id:"1000"){
        titre
        auteur {
            nom
        }
    }
}
\end{lstlisting}

Les résultats de cette dernière requête sont illustrés dans le Listing X. Comme vous pouvez le constater, le résultat est un objet au format JSON, qui doit être analysé et éventuellement désérialisé par les clients qui ont demandé les champs "titre" et "corps". En résumé, GraphQL permet aux clients de définir précisément les données qu'ils souhaitent obtenir, ce qui réduit le gaspillage de bande passante et simplifie le processus de récupération des informations nécessaires.

\break
\begin{lstlisting}
{ 
    "data": {
        "publication":{
            "titre": "..."
            "auteur":{
                "nom": "..."
            }
        }
    }
}
\end{lstlisting}

Nous pouvons constater que GraphQL résout 2 problèmes qui sont sur-télécharger et sous-télécharger. Sur-télécharger est quand le client fait une requête et reçoit une réponse du serveur contenant des informations superflues, ce qui force le client à implémenter la logique de son côté pour les filtrer. La conséquence est qu'une grosse quantité de données inutiles est transmise, gaspillant la bande passante et potentiellement augmentant le temps de transmission, d'encodate et de décodage. Sous-télécharger est le phénomène inverse, quand le point d'accès au service ne fournit pas suffisamment d'information et donc force le client à faire plusieurs requêtes pour récolter les ressources rechercher. Le problème de sous-télécharger peut être réglé facilement en suivant plusieurs points d'accès en une seule requête. Régler le problème de sur-télécharger, en revanche, demande des techniques plus sophistiquées, car le besoin de filtrage peut facilement changer d'une application à l'autre. \cite{Vadlamani2021}



GraphQL est relativement nouveau par rapport à REST. Brito et al \cite{Brito2019} ont examiné un article de blog (littérature grise) pour comprendre les avantages et les caractéristiques clés de GraphQL, tels que perçus par les praticiens. Ils concluent que GraphQL est plus efficace en termes de réduction de 99\% de la taille des documents JSON (en termes de nombre d'octets). Hartig et Perez \cite{hartig2017initial}, \cite{hartig2018semantics} fournissent la sémantique d'une requête formelle et analysent ensuite le langage. Leurs résultats montrent que GraphQL est un langage relativement moins complexe. Wittern et coll. \cite{wittern2018generating} a évalué la faisabilité de encapsuler automatiquement GraphQL pour les API REST (de type) existantes avec la spécification Open API (OAS) et sur cette base, ils ont étudié les gains obtenus entre GraphQL et REST. En outre, ils discutent des défis liés à la création de tels encapsuleur, tels que l'assainissement des données, l'authentification ou le traitement des requêtes imbriquées.

En ce qui est question des outils de traçage très peu intéragisse avec une base de donnée, mais plutôt au fichier de trace directement. Il reste que certain emmagasine les données traces dans un certain format et l'expose avec une base de donnée. Un exemple concret serait PromQL. PromQL est le langage de requête exposé par le serveur Prometheus pour l'accès aux traces sauvegardées. La base de données de Prometheus est organisée sous forme de séries temporelles. Par conséquent, une requête trivial consiste à extraire le point de données enregistré pour chaque instant dans le temps. Ensuite, pour filtrer la réponse, il est possible de spécifier les étiquettes (mots-clés) nécessaires. Ainsi, seuls les points de données dotés de ces étiquettes seront renvoyés. De plus, Prometheus prend en charge divers opérateurs d'agrégation. Par exemple, si l'on souhaite calculer la somme des points de données renvoyés, cela peut être réalisé grâce à ces opérateurs d'agrégation.

\subsection{Remote Procedure Call}

Les Remote Procedure Call (RPC) permettent à une machine A d'appeler des procédures sur une machine distante B, comme si elles étaient disponibles dans l'espace d'adressage local \cite{Coulouris_2012}.

JSON-RPC est un protocole RPC sans état, qui utilise le format JSON comme format d'échange et est conçu pour être simple \cite{JsonRpcSpec}. Une requête du client contient les champs suivant:
\begin{itemize}
    \item jsonrpc: Permet de spéciﬁer la version du protocole;
    \item method: Contient le nom de la méthode à exécuter;
    \item params: Permet de lister les valeurs des paramètres de la méthode;
    \item id: Permet d'associer une réponse du serveur à la requête du client.
\end{itemize}
Une requête doit toujours recevoir une réponse. Le protocole supporte également les notiﬁcations, un message qui n'a pas besoin d'avoir de réponse. Finalement, JSON-RPC ne spéciﬁe pas la couche de transport par laquelle les messages sont acheminés.

gRPC (Google RPC) est une infrastructure multiplateforme, indépendante du langage et de la plate-forme, à usage général, utilisée par Google Inc. et rendue publique en 2015. Elle peut générer automatiquement des stubs client et serveur idiomatiques pour un service dans une variété de langues et plates-formes. Il utilise des tampons de protocole qui constituent un mécanisme flexible, efficace et automatisé pour la sérialisation binaire des données structurées. L'utilisateur déﬁnit des services dans un ﬁchier .proto et le compilateur protoc génère les API qui seront utilisés par le client et le serveur.

gRPC offre une meilleure performance que le protocole JSON-RPC. \cite{kiraly2018analysing} Cette différence de performance est causé par le format de transmission. JSON-RPC utilise un format textuel (JSON) pour la transmission et gRPC utilise un format binaire (Protobuffers). Le format binaire résout à un encodage plus petit et accélère la phase de décodage puisque qu'il n'a aucune conversion nécessaire. Le temps de transmission réseau se voit également réduit. Cela donne un avantage à gRPC lorsque la performance est un critère important.


\subsection{Language Server Protocol}

Le Language Server Protocol (LSP) est un protocole de communication basé sur JSON-RPC, développé par Microsoft, qui permet l'interaction entre un éditeur ou un environnement de développement intégré (EDI) et un serveur de langage de programmation \cite{Keidel2016}. Un serveur de langage fournit diverses fonctionnalités telles que l'autocomplétion de code, la coloration syntaxique, la navigation vers la définition, et la signalisation des erreurs dans le code. L'objectif principal du LSP est de standardiser la manière dont ces fonctionnalités sont mises à disposition des EDI. Cela permet de rendre l'implémentation d'un EDI indépendante du serveur de langage, favorisant ainsi la réutilisation des serveurs de langage dans différents EDI. Initialement conçu pour VS Code, le LSP a évolué grâce à la contribution d'organisations telles que Red Hat et Codeenvy. Plusieurs entreprises, dont la fondation Eclipse, Github, Sourcegraph et Red Hat, ont adapté leurs outils de développement populaires, tels qu'Eclipse, Atom, IntelliJ, Emacs, Sublime, pour prendre en charge le protocole. Actuellement, plus de 200 langages de programmation, y compris Java, C/C++, C\#, PHP, Python et JavaScript, prennent en charge le LSP.

Le LSP définit trois types de messages : les requêtes, les réponses et les notifications. Les requêtes sont initiées par le client en utilisant des appels de procédures JSON-RPC, et le serveur répond à chaque requête par une réponse. Les notifications peuvent être envoyées par le serveur ou le client, sans le retour d'une réponse. La figure X ci-dessous illustre un exemple d'interaction et d'échange de messages entre un EDI et un serveur de langage pendant une session d'édition de code. De plus, le LSP supporte la cancellation de requête et le rapport de progression. Grâce à cela, il est possible de canceller des requêtes en cours et d'obtenir des réponses partielles.

    [Figure https://microsoft.github.io/language-server-protocol/overviews/lsp/img/language-server-sequence.png ]

Les interactions entre le client et le serveur se basent sur des modèles indépendants du langage de programmation et de l'EDI. Par exemple, lorsqu'un client souhaite accéder à la définition d'une procédure, il échange un Uniform Resource Identifier (URI) avec le serveur pour localiser un document sur disque et une position dans ce document. Le LSP permet également au serveur de signaler quelles fonctionnalités il prend en charge, laissant au client la gestion de la durée de vie du serveur.

Monto est une architecture qui connecte un EDI à des composants offrant des fonctionnalités de langage, adoptant une approche orientée service. Chaque fonctionnalité de langage, comme l'autocomplétion, est fournie par un service qui peut être décomposé en sous-services. L'architecture permet à l'EDI de connaître les fonctionnalités disponibles enregistrées auprès d'une composante appelée broker. Un module d'extension dans l'EDI gère la communication avec le broker. La figure X illustre une vue d'ensemble de l'interaction entre les composants de l'architecture Monto.

    [Figure de l'architecture Monto avec un broker]


L'architecture Monto définit deux types de messages : les messages source et les messages produit. Les messages source, envoyés du module d'extension de l'EDI vers le broker, contiennent un identifiant unique, le nom du fichier, le langage de programmation et le contenu du fichier. Le broker distribue les messages source aux services appropriés et répond au module d'extension de l'EDI lorsque les opérations sont terminées. Les messages produits encapsulent une représentation intermédiaire générique, indépendante du langage et de l'EDI. Les communications sont effectuées via ZeroMQ, une bibliothèque de messagerie asynchrone haute performance, avec l'encodage des messages au format JSON. Cette approche permet d'avoir un serveur sans état et n'exige pas de conserver une copie des documents ouverts, contrairement au LSP. Cependant, l'envoi du contenu complet d'un fichier au format JSON peut ne pas être optimal en termes de transfert de données sur le réseau.


\subsection{Language Server Index Format Specification}

Language Server Index Format (LSIF) est un protocole et un format de données utilisés dans le cadre du LSP. LSIF a été développé pour améliorer les performances et l'efficacité de la communication entre l'éditeur de code et le serveur de langage. Il s'agit de générer et de stocker un index des informations du projet source, ce qui permet de réduire les délais de réponse lors de l'interaction avec le code source.

1. Génération de l'index : Le serveur de langage génère un index LSIF en analysant le projet source. Cet index contient des informations sur la structure du code, les définitions de fonctions, les variables, les références croisées, etc.

2. Stockage de l'index : Une fois généré, l'index LSIF est stocké dans un fichier ou une base de données. Ce fichier est souvent enregistré à un emplacement spécifique du projet.

3. Utilisation par l'éditeur de code : Lorsque l'utilisateur interagit avec le code source dans l'éditeur, celui-ci peut interroger l'index LSIF pour obtenir des informations pertinentes, telles que des suggestions de complétion, la documentation, ou pour effectuer des actions de refactoring. Cela permet à l'éditeur de code de répondre plus rapidement aux demandes de l'utilisateur, car il n'a pas besoin de re-analyser tout le projet à chaque interaction.

LSIF est particulièrement utile pour les projets de grande envergure où l'analyse statique du code peut être coûteuse en termes de ressources. En utilisant un index pré-généré, les performances de l'éditeur de code s'améliorent significativement.

En résumé, LSIF est un format d'indexation utilisé dans le cadre du protocole LSP pour améliorer les performances de l'interaction entre les éditeurs de code et les serveurs de langage lors de la programmation. Il permet de stocker des informations structurées sur le code source, ce qui facilite la fourniture de fonctionnalités avancées aux développeurs.


\subsection{Trace Server Protocol}

Le TSP a été développée lors de l'introduction de l'architecture client-serveur dans Trace Compass par Chen Kuang Piao en 2018. Ce protocol suit la philosophie des REST API et utilise actuellement JSON comme format d'échange de données. Cependant, le travail de Chen Kuang Piao a suggéré qu'il serait possible d'obtenir des améliorations significatives en termes de taille des messages et d'efficacité de sérialisation/désérialisation en utilisant Protobuf à la place. En sachant que la sérialisation/désérialisation devient un facteur très important dans le temps de réponse lorsque nous essayons de traiter des traces énormes [Article de Hao]. Il devient alors très important d'évaluer les solutions existantes qui nous permettrait de réduire l'impact de la sérialisation/désérialisation. Un impact qui devient encore plus important lorsque nous essayons de transitionner vers une architecture distribué qui risque de nécessité plusieurs communications réseaux, et donc plusieurs sérialisations/désérialisations pour une seule requête.

La transition vers un autre protocol est envisageable, car REST, en tant qu'approche, n'est pas lié à un format de données spécifique, mais plus au ressource accessible par les différentes d'accès. En plus des points d'accès permettant de récupérer des données de traces ou de sessions d'analyse, les points d'accès pour obtenir le modèle de visualisation des traces sont organisés par type de visualisation. Il existe des points d'accès dédiés aux analyses sous forme de graphes XY, au graphe temporel et sous forme de trableau. À savoir que le protocol est en pleine évolution. Alors il est important de considérer que le protocole risque de divaguer de ses objectif primaire en terme de principe, comme le standard RESTful.

Dans le protocole TSP, il est possible d'ajuster la résolution des données, ainsi que les items souhaitées en spécifiant les points de temps souhaités en tant que paramètres. Cette pratique n'est pas courante dans les normes REST qui n'impliquent généralement pas de filtrage des réponses dans le corps de la requête, mais plutôt dans les paramètres de la requête. À exception, nous retrouvons des paramètres de filtrage dans le corps de la requête lorsque nous rencontrons un problème lié à la limite de caractère possible dans un URL.

% (Chen Kuang Piao, 2018) [16] \cite{chen2021distributed}
% (Ericsson) [42] \cite{Ericsson_TSP_OPENAPI}
% [41] \cite{github_ericsson_TSP}




\section{Conclusion de la revue littéraire}

À travers cette revue de littérature, nous avons vu qu'il a eu beaucoup travail effectuer pour pallier au problème de récolter, d'analyser et de visualiser des traces. Toutes ces applications proposent des modèles bien précis à leur besoin avec la mise à l'échelle en tête. Cela permet d'identifier ce qui semble marcher ou ce qu'il faudrait améliorer. Une lacune évidente des outils de traçage est dû au manque de standardisation ce qui rend le flux de travail instrumentation-collection-analyse-visualisation difficile. Par chance, le protocole TSP offre une fondation pour pouvoir offrir un standard robuste pour favoriser la réutilisation des serveurs d'analyse pour les différents outils de visualisation. Cependant, le protocole n'avait pas la mise à l'échelle en tête.

En conséquence, nous souhaitons d'abord d'évaluer les limites du protocole TSP sur des grappes de calcul. Nous souhaitons ensuite proposer une extension aux protocoles pour permettre aux serveurs d'implémenter des analyses plus complexe comme le chemin critique.


