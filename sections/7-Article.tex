\Chapter{Article 1}\label{sec:architecture}

\section{Introduction}

\section{Related Work}

The related work is divided into six sections. The ﬁrst section will report recent works on Tracing Analysis. The second section will report recent works on integrated development environment. The third section will present different distributed artchitecture that application use for scaling. The fourth section will present recent works on distributed analysis. The fifth section will prensent recent works on dsitributed critical path. The sixth section will present recent works on Theia.


\subsection{Tracing Analysis}

 Tracing involves a specialized use of logging that records runtime events about a program's execution. Tracing is used to detect and identify performance issues, behavior issues and security issues (FIND SOURCE HERE). Tracing can be done on several levels including user-space, kernel, hardware, hypervisor, network, etc. Software that capture those events is called a tracer. To capture those events, tracepoints must be inserted either statically or dynamically. In the case that the tracepoint is inserted statically, the code must be modified to include tracing macros and must be recompiled. In the case that the tracepoint is inserted dynamically, tracepoints are added dynamically to a compiled and running program (Gregg and Mauro, 2011). During the execution of the program, events will be emitted and a tracer will capture them. At this point, a tracer will produce a trace file, organized in a specific format. Because the tracer have to run at the same time of the main application, it will impact the execution time of the main application. This is why a tracer must have a minimal execution overhead. When the tracer finish to produce a trace file, we can apply a trace analysis.

 A trace analysis transforms the trace events into a dataset to perform more investigation and it reorganizes them into data structure for faster access (Prieur-Drevon et al., 2018).

 \subsection{Integrated Development Environment}
 
 % IDE evolution: No plugins, Plugins, Server Protocol, Desktop base IDE, Cloud base IDE, Hybrid base IDE
Integrated Development Environments (IDEs) have played a pivotal role in shaping the landscape of software development. These powerful software tools have evolved significantly over the years, revolutionizing the way developers write, debug, and maintain code. These IDEs greatly enhanced developer productivity and set the stage for more sophisticated tools. IDEs like Microsoft Visual Studio provided comprehensive solutions for building complex software applications. Developers could now easily navigate through code, leverage libraries, and visually design user interfaces within a single environment. It is clear those IDEs help developers to be more and more productive, depending on the developing phase. The developers get a lot of support from IDEs. The problem is, each time that we add features in IDEs, it become more harder for developer to reach the  productive stage [Article 5]. That is why developers start using IDEs that have basic feature of an text editor and enhance it, using plugins and extension, to become a full featured IDE [Article 6]. 


% Different Server Protocol aujourd'hui mis en place
A lot of work as been put to decouple the support of programming language such as code completion, code error, syntax highlighting and go-to definition. The objective was to enable the support of many programming language using language service [Article 8]. Microsoft's language server protocol (LSP) is a communication protocol, based on JSON-RPC, between a client, which is the IDE, and a server that offers language support. The protocol provides common features such as code completion, code error, syntax highlighting and go-to definition and many more. Many organizations such as the Eclipse Foundation and JetBrains are adapting or creating IDE (Eclipse, IntelliJ) to implement the LSP [Article 7]. 

In the same way, a lot of work as been put to decouple the debugging tools and the IDEs. Microsoft's debugger adapter protocol (DAP) is implement in Visual Studio Code and Theia. The base protocol exchanges messages that consist of a header and a content part (comparable to HTTP). Josselin et al. proposed a protocol that supports meaningful Domain-Specific Languages (DSLs) and that can be used with minimal effort within an IDE that implement DAP. This debugger allowing a generic interactive debugger to communicate with heterogeneous DSL runtime [Article 10]. 

Marr et al. have presented the Kómpos protocol [Article 9] which proposes a concurrency-agnostic debugger protocol that decouples the debugger from the concurrency models employed by the target application. As a result, the underlying language runtime can define custom breakpoints, stepping operations, and execution events for each concurrency model it supports, and a debugger can expose them without having to be specifically adapted. In comparison to existing debugger protocol such as Java Debug Wire Protocol (JDWP) or the GNU Debugger (GDB) machine interface or the Chrome DevTools protocol or Visual Studio Code debug protocol, Kómpos protocol solution is not specific to a concurrency concept.

\subsection{Scaling Distributed Architecture}

Modern Internet services are often implemented as complex, large-scale distributed systems. These applications are constructed from collections of software modules that may be developed by different teams, perhaps in different programming languages, and could span many thousands of machines across multiple physical facilities. Tools that aid in understanding system behavior and reasoning about performance issues are invaluable in such an environment.

Artificial Intelligence (AI) and cloud computing has emerged as a promising avenue for addressing the growing computational demands of AI applications. Parallel and distributed training of AI models become increasingly complex. Parallel and distributed training techniques have emerged as essential approaches to reduce training time and improve resource utilization. By leveraging these techniques, researchers and practitioners can accelerate the training process, enhance model performance, and reduce costs associated with cloud-based AI systems[Article 11].

Resource demands of HPC applications vary significantly. The varying resource demands can lead to HPC resources being not fully utilized. It becomes very difficult for HPC systems to find the reasons for throttling jobs. [Article 13]

[Article 14]



\subsection{Distributed Analysis}

% Indeed, trace ﬁles could easily contain millions, even billions of events and the analysis must use an efficient data structure to maintain query performance

Spark and Kafka and Prometheus

% Rocmon monitoring system is a prototype. This prototype extends Nagios monitoring solutions. Rocmon aims at a scalable monitoring infrastructure, flexible to accommodate provider's needs, and open to accept user requests. The prototype extends the flexibility of the Nagios plugin approachthe OCCI API to be able 


% Distributed tracing does not necessarily require high scalability. Some distributed systems are compact, due to physical constraints, and stay relatively small. It is probably why Nagios, one of the most popular surveillance platforms for distributed applications (Tamburri et al, 2020) [27] suffered from scalability issues. The tool laid out a pattern for collecting and analysing distributed trace data, as well making itself extendable with a plug-in system, allowing connections to third party services (Renita et Elizabeth, 2017) [28]. Nonetheless, the fact that there is only one central processing instance holds it from going any further. There were numerous works aiming at solving this issue (Ciuffoletti, 2016) [29] by embedding multiple Nagios instances, thus extending its capabilities. 

% The recent work for parallelizing trace analysis (Reumont-Locke, 2015) [17], (Martin, 2018) [18] shows attempts at improving the efficiency of the process. This alone, however, is not sufficient to address the scalability issue in distributed environments. As efficient as it can ever be, a single instance of a trace analysis tool is bound by the computational power 32 of the underlying machine, which in extreme cases is at best as powerful as the traced one. Multiplying the traced machine by hundreds or thousands of nodes, we quickly overwhelm the analysis tool. This contention point would be problematic for applications where 24/7 uptime is wanted. An anomaly can escalate quickly in clusters (Matloff) [20] and a delay in analysing its data during investigation does not help. A similar pattern found among distributed tracing platforms consists of multiple tracers, each running on a traced machine, all sending its trace data to a central collector for analysis work. In some cases, the tracer takes the form of a client library to create custom events with specific information related to the application, in contrast to the black-box tracing nature of Nagios. Prometheus achieves scalability by making it possible to organize Prometheus instances into a federation (Reback, 2021) [32]. Indeed, it is possible to set up a multi-level hierarchy of Prometheus servers, each level reporting aggregated data to the upper level. This approach was in fact also used by Nagios, later on, to overcome its own scalability issue (Nagios) [30]. Scaling analysis servers horizontally is a good approach. However, Prometheus analyses are simple, with mostly metrics, and thus the aggregation is viable. Attempting to apply the same model for more sophisticated analysis processes, such as the ones that Trace Compass offers, would be difficult. Their aggregations are highly different, from one type to another. 

% Scalability is not the only challenge facing distributed tracing. Merging trace data from different machines is not a trivial task either. Trace Compass allows developers to define how to select events to correlate with causality links, in order to time align and merge different sets of trace data together, and follow dependency links for the critical path computation. Linking together traces at different levels like this is somewhat context dependent. For instance, it has been implemented in Trace Compass to link traces of processes on different nodes exchanging TCP packets, to link traces from KVM virtual machines interacting with their Linux host trace, and also to link traces from different processors on heterogeneous systems. However, a specific implementation is required for each type of application (Ericsson) [15]. Traditional tracers are not aware of execution context, and inferring the context during the analysis phase is also difficult, due to the lack of necessary details in the trace data (Uber, 2017) [2]. The most adopted solution for this problem is called context propagation, which is the technique employed by Jaeger. The idea is to include a unique request identifier in every event generated, and to link together requests and their nested requests. This approach is more invasive, because of the need to generate and propagate requests identifiers in the middleware libraries, but is more reliable and lower overhead as compared to techniques like "Black-box inference" or "Schema-based" (Shkuro, 2019) [21].


\subsection{Distributed Critical Path}

\subsection{Theia}

Grafana
%Eﬀtinge and Kosyakov have presented Theia (Eﬀtinge and Kosyakov, 2017), a new open-source IDE framework for building IDEs that could run both as a desktop application or in a web browser connected to a remote backend. The project shares a lot of similarities with Microsoft's code editor, Visual Studio Code. Theia uses Node.js, Electron and implements the LSP
\section{Proposed Solution}
\subsection{Overall architecture}
\subsection{}
% \subsection{Connect logically the traces}
% \subsubsection{What are the available traces}
% \subsubsection{Where are the traces}
% \subsubsection{How to connect the traces}

\subsection{Distributed Analysis}

\subsubsection{Aggregation of independent trace (vertically)}

\subsubsection{Critical Path}

\subsection{Visualization}

\section{Test Environment}

\subsection{Computer Specification}

\subsection{Generate Config file}

\subsection{Deployment}

\section{Results}

\subsection{Trace API with(out) Coordinator}

\subsection{Experiment API with(out) Coordiantor}

\subsection{Timegraph API with(out) Coordiantor}

\subsection{Xy Graph API with(out) Coordiantor}

\subsection{Virtual Table API with(out) Coordiantor}

\subsection{Critical Path Analysis with(out) Coordiantor}

\section{Discusssion}

\section{Conclusion}

\section{Acknowledgement}