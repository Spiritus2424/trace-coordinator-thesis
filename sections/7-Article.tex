\Chapter{Article 1}\label{sec:architecture}

\section{Introduction}

\section{Related Work}

The related work is divided into six sections. The first section detail recent work on Tracing Analysis. The second section report recent work on integrated development environment. The third section present different distributed architecture that application use for scaling. The fourth section expose recent work on distributed analysis. The fifth section report recent work on distributed critical path. The sixth section present recent work on Theia.

\subsection{Tracing Analysis}

Tracing involves a specialized use of logging that records runtime events about a program execution. Tracing is used to detect and identify performance issues, behavior issues and security issues (FIND SOURCE HERE). Tracing can be done on several levels including user-space, kernel, hardware, hypervisor, network, etc. Software that capture those events is called a tracer. To capture those events, tracepoints must be inserted either statically or dynamically. In the case that the tracepoint is inserted statically, the code must be modified to include tracing macros and must be recompiled. In the case that the tracepoint is inserted dynamically, tracepoints are added dynamically to a compiled and running program (Gregg and Mauro, 2011). During the execution of the program, events will be emitted and a tracer will capture them. At this point, a tracer will produce a trace file, organized in a specific format. Because the tracer have to run at the same time of the main application, it will impact the execution time of the main application. This is why a tracer must have a minimal execution overhead. When the tracer finish to produce a trace file, we can apply a trace analysis.

Linux itself includes perf \cite{kernelPerfWiki}, a set of tools that utilize the perf\_event\_open \cite{man7Perf} infrastructure. These powerful tools cover a wide range of performance related information that is available to the operating system. Available features include performance counter based sampling, certain instrumentation points, logging to record the collected information, and visualization as textual profiles. However, perf uses a monolithic file-format and lacks a scalable and user-friendly way to analyze and visualize the resulting timelines. Previously, we have used perf for performance monitoring and converted the resulting trace into a different file format that allowed the usage of timeline visualization tools \cite{schone2014scalable}.

OProfile \cite{sourceforgeAboutOProfile} is a non-intrusive statistical profiler built on the perf\_event\_open infrastructure. It supports low-overhead system-wide and single process monitoring, but is limited to aggregated profiles only.

A trace analysis transforms the trace events into a dataset to perform more investigation and it reorganizes them into data structure for faster access \cite{prieur2018r}.

HPCToolkit \cite{adhianto2010hpctoolkit} is a performance analysis tool suite that focuses on sampling of parallel applications, supporting MPI and OpenMP. It uses summarization to provide performance profiles. Moreover, a timeline view based on logging is available. HPCToolkit includes information from call stack samples which are processed in userspace with sophisticated unwinding techniques and performance application programming interface performance counters.

Trace Viewer is an open-source extension available in the Theia IDE. It use the Trace-Server-Protocol to query data models and visualize different trace analysis. A trace analysis reads a trace sequentially and stores its results (called state models) in a data structure that can be written on disk. From the trace events, the analysis would extract the intervals of the state values for different system resources (CPU, processes, disks, etc.). \cite{chen2021distributed}

\subsection{Integrated Development Environment}

A lot of work as been put to decouple the support of programming language such as code completion, code error, syntax highlighting and go-to definition. The objective was to enable the support of many programming language using language service \cite{Keidel2016}. Microsoft language server protocol (LSP) is a communication protocol, based on JSON-RPC, between a client, which is the IDE, and a server that offers language support. The protocol provides common features such as code completion, code error, syntax highlighting and go-to definition and many more. Many organizations such as the Eclipse Foundation and JetBrains are adapting or creating IDE (Eclipse, IntelliJ) to implement the LSP \cite{Bunder2019}. 

In the same way, a lot of work as been put to decouple the debugging tools and the IDEs. Microsoft debugger adapter protocol (DAP) is implement in Visual Studio Code and Theia. The base protocol exchanges messages that consist of a header and a content part (comparable to HTTP). Josselin et al. proposed a protocol that supports meaningful Domain-Specific Languages (DSLs) and that can be used with minimal effort within an IDE that implement DAP. This debugger allowing a generic interactive debugger to communicate with heterogeneous DSL runtime \cite{enet2023}. 

Marr et al. have presented the Kómpos protocol \cite{Marr2017} which proposes a concurrency-agnostic debugger protocol that decouples the debugger from the concurrency models employed by the target application. As a result, the underlying language runtime can define custom breakpoints, stepping operations, and execution events for each concurrency model it supports, and a debugger can expose them without having to be specifically adapted. In comparison to existing debugger protocol such as Java Debug Wire Protocol (JDWP) or the GNU Debugger (GDB) machine interface or the Chrome DevTools protocol or Visual Studio Code debug protocol, Kómpos protocol solution is not specific to a concurrency concept.

% \subsection{Scaling Distributed Architecture}

% Modern Internet services are often implemented as complex, large-scale distributed systems. These applications are constructed from collections of software modules that may be developed by different teams, perhaps in different programming languages, and could span many thousands of machines across multiple physical facilities. Tools that aid in understanding system behavior and reasoning about performance issues are invaluable in such an environment.

% Artificial Intelligence (AI) and cloud computing has emerged as a promising avenue for addressing the growing computational demands of AI applications. Parallel and distributed training of AI models become increasingly complex. Parallel and distributed training techniques have emerged as essential approaches to reduce training time and improve resource utilization. By leveraging these techniques, researchers and practitioners can accelerate the training process, enhance model performance, and reduce costs associated with cloud-based AI systems[Article 11].

% Resource demands of HPC applications vary significantly. The varying resource demands can lead to HPC resources being not fully utilized. It becomes very difficult for HPC systems to find the reasons for throttling jobs. [Article 13]

% [Article 14]



\subsection{Distributed Analysis}

Recent efforts to parallelize trace analysis, as exemplified by the work of Reumont-Locke (2015) \cite{reumont2015methodes} and Martin (2018) \cite{Martin2018}, have aimed to enhance the efficiency of this process. Nevertheless, optimizing efficiency alone cannot fully address the scalability challenges inherent in distributed environments. Even if a trace analysis tool operates at peak efficiency, it remains constrained by the computational capabilities of the underlying hardware, typically equivalent to the traced system in the best-case scenario. When we scale up the number of traced nodes to hundreds or thousands, the analysis tool quickly becomes overwhelmed. This bottleneck becomes particularly problematic in applications requiring continuous uptime, as anomalies can rapidly escalate within clusters (Matloff) \cite{matloff2011programming}, and delays in data analysis during investigations only exacerbate the situation.

A similar approach observed in distributed tracing platforms involves deploying multiple tracers, each running on a traced machine, with all trace data being transmitted to a central collector for subsequent analysis. In some instances, these tracers take the form of client libraries, allowing for the creation of custom events with application-specific information, in contrast to the more opaque black-box tracing approach of tools like Nagios. Prometheus addresses scalability concerns by enabling the organization of Prometheus instances into a federation, as demonstrated by Reback in 2021 \cite{Logz.io_prometheus_2023}. This approach facilitates the establishment of a multi-level hierarchy of Prometheus servers, with each level aggregating and forwarding data to the level above. Notably, Nagios also adopted a similar approach to mitigate its own scalability challenges (Nagios) \cite{Nagios2019}.

While horizontally scaling analysis servers is an effective strategy, it is essential to recognize that Prometheus analyses primarily consist of simple metrics, making aggregation feasible. However, applying the same model to more complex analysis processes, such as those offered by Trace Compass, presents substantial challenges. The nature of their aggregations varies significantly between different analysis types, making direct application of the Prometheus approach difficult in such cases.

\subsection{Distributed Critical Path}

Recent efforts to parallelize critical path analysis, as exemplified by the work of Denis and Dagenais, have aimed to enhance the efficiency of this process. The kernel events necessary for analysis execution are identified. A distributed architecture that did not require any file transfer between the client and traced nodes, and allowed the distribution of trace analysis computation is presented. They also explore the algorithm to resolve the remote dependencies \cite{matloff2011programming}.

Although distributed architectures offer a solution to the scalability problem, it is challenging to fully leverage their benefits due to network communication when broadcasting to all nodes. This is why restricting communications solely to the node essential for the calculation is crucial \cite{denys2023distributed}.


\subsection{Theia - Trace Viewer Extension}

Eﬀtinge and Kosyakov have presented Theia (Eﬀtinge and Kosyakov, 2017), a new open-
source IDE framework for building IDEs that could run both as a desktop application or in
a web browser connected to a remote backend. The project shares a lot of similarities with
Microsoft code editor, Visual Studio Code. Theia uses Node.js, Electron and implements
the LSP.


Theia Trace Viewer extension is the newly developed server-client application developed by Ericsson and the Eclipse Foundation, along with other contributors. It purpose is to replace Trace Compass as the Eclipse general-purpose tracing tool. As previously described Trace Viewer uses a server-client architecture, where the client communicates with the server through a protocol called Trace Server Protocol (TSP) \cite{desnoyers2006lttng}.

\section{Proposed Solution}


\subsection{Motivation \& Background}

Currently, we have several problems related to trace analysis of distributed systems. It is difficult to be able to group all the traces on one machine. If the tracker does not store traces on a machine as Jaeger does, this must be managed by the user. This becomes a problem when we have 1000 and more compute nodes. This adds a workload and sometimes even becomes very limiting. For a small trace of 1GB on each node for 1000 nodes, hard disk space quickly becomes an issue for analyzing distributed system traces.

Trace Compass excels in its ability to support various trace formats, making it a standout framework in this regard. This platform offers a seamless way to incorporate custom front-end parsers and even introduce novel types of analyses. Notably, the separation between the Trace Compass client and server components positions it as a promising option for distributed deployment. While the current Trace Compass server can communicate with a remote client through the Trace Server Protocol, it has limitations. It can only run as a single instance, making it inadequate for handling substantial volumes of trace data in environments like HPC clusters. This limitation becomes apparent when considering the demands on CPU, storage, and RAM memory resources. Additionally, the Trace Compass server presently requires all trace data to be stored locally for analysis, which may not align with certain use cases.

Certainly, running multiple Trace Compass servers, each dedicated to assessing the traces of individual machines within a cluster, is a feasible approach. However, it is crucial to emphasize the importance of having an overarching view of the entire cluster. To make Trace Compass more adept at tracing extensive and diverse systems, a fundamental re-architecture with scalability in mind is imperative. Our aim is to equip Trace Compass with robust query capabilities akin to those found in established platforms, all while preserving its inherent flexibility for the development of novel analysis types and trace formats. Achieving this objective necessitates the implementation of an abstract layer, allowing for the creation of new query operators with minimal effort, while maintaining the overall behavior of the platform. This concept aligns with the notion of the trace parser or the DataProvider layers, which provide an abstraction of the view, irrespective of the specific type of analysis within Trace Compass.


\subsection{Overall architecture}

The architecture platform consists of three main parts, the traced nodes, the analysis nodes and the coordinator node. An analysis node serves as the designated location for processing trace data that originates from traced nodes. This configuration enable us to directly deploy the analysis node where the traces are which is more easy that move the traces at the same node. Each individual analysis node is tasked with handling a distinct set of traced nodes. In our particular configuration, we designate each analysis node as a Trace Compass Server. The coordinator node plays a pivotal role when a client or view initiates a query. It, in turn, dispatches queries to the relevant analysis nodes and subsequently combines their responses to furnish an integrated analysis outcome. When a client transmits a request to a server seeking trace analysis data, the predominant portion of the response typically comprises data points, frequently presented in the form of a time series array.

The need to have a protocol that allows the feasibility or potential to construct complex operations is not necessary in our situation. The reason is because the flexibility of having complex operations is already included in the flexibility of creating analyzes in the Trace Compass architecture. Therefore, we opted for the Trace Server Protocol (TSP) as the communication protocol for both trace data and analysis requests. Several factors guided this decision. Firstly, TSP already possesses the functionality to filter data points based on timestamp parameters, and it can be readily extended to incorporate additional analysis base on current view. Secondly, the Trace Compass server, which we selected for our prototype implementation, already supports this protocol. Thirdly, TSP is designed to break away from existing analytics implementations and focus on displaying data. This allows us to keep the flexibility that Trace Compass has to add or modify analyzes, but also limits the computing power needed on the coordinator, because the coordinator will mainly be aggregating the data together and we want to focus what is compute intensive at analysis node. As a bonus, it help us to easily maintain the coordinator because the aggregation will be base on the views available in the protocol and not the implementation.

[Mettre une figure]

\subsection{Distributed Analysis}

Within the TSP protocol, analysis is restricted to operating on experiments. An experiment is essentially a compilation of all the desired traces required for a particular analysis. Presently, TSP provides four distinct views models for displaying the results of an analysis, depending on the data provider available in the backend: Table, XY Graph, Time Graph, and Data Tree. These models allow the view to directly display data to the user, without prior processing. This makes it possible to avoid resource constraints that certain workstations or applications might have and make the most of the remote resources made available.

In these views, with the exception of virtual table, there are two key axes: the vertical and horizontal axes. The vertical axis represents all the processes within the trace and is visually represent in rows, while the horizontal axis signifies time and is visually displayed in columns.

When an experiment is created and traces are added to it, if the traces are independent, they merely contribute additional rows to the basic analysis, thereby extending the data along the vertical axis. However, if the traces originate from the same tracing session and this session is segmented into multiple trace parts, as exemplified in "Lttng Log Rotation" [Reference], the data can expands both along the vertical and horizontal axes.

In certain scenarios, specific analyses may necessitate access to raw data for completing the analysis, such as the critical path analysis. Regrettably, the TSP does not currently provide any endpoints for retrieving this unprocessed data. Consequently, we have proposed a solution to extend the capabilities of the TSP, enabling users to access data that has not undergone processing for the various views and at the same time allow more feature that could be useful for users in the future.

\subsubsection{Aggregation of independent trace (vertically)}

TSP conveniently exposes the analysis results in the form of a view model data. As previously discussed, this approach offers the advantage of enabling the client to directly display the data without the need for additional processing. Consequently, our proposed solution focuses its aggregation efforts on this preformatted result, as opposed to the raw data retrieved when querying the state system, as is the case with Trace Compass.

Most of the analyses are currently represented as a series of data points over a timeline. Therefore, when requesting an analysis result via TSP, it primarily necessitates providing the timestamps for the analyzed timeline and the requested items for the analyzed process that we want to show [Example query]. Upon receiving requests from the client or view, the coordinator node can efficiently retain the request in its original form, duplicate it, and broadcast it to the other nodes. Subsequently, it aggregates the returned responses in a cohesive manner.

This approach works well because we are analyzing traces from different machines, generated independently but during the same time frame. As a result, the querying time interval remains consistent. In essence, we can liken this process to a Map/Reduce paradigm, with the distributed data processing involving the analysis nodes acting as the workers during the Map phases and the coordinator node overseeing the Reduce phases. Upon receiving a request, the coordinator queries the analysis nodes and then merges all their responses, to produce the final response for the client. 

Let us take a use case that, when aggregating data from multiple sources, computes the ratio of each value to the "sum" of data (e.g. percentage of each CPU usage). Because the trace are independent and make a percentage base on the resources of the machine that the trace was collected, we don't need to recalculate the ratio of each value. 

The problem is that each analysis nodes are independent and don't have the notion of what other nodes could produce. Each rows that is display in the view are suppose to have an unique identifier and not an universal unique identifier. So each node analysis when they are generating the data. It is incrementing a counter for each rows that is create that start from 0. Therefore, when the coordinator will aggregates the response from each analysis nodes, it will send back to the client/views many series of data points with the same identifier. To solve this problem, the coordinator will take responsibility to change the identifier of each rows. At the start, the coordinator will take into account how many analysis node is available and assign a rank between $0 to n-1$. Where $n$ is the number of analysis nodes. Then each query that is made to an analysis node will have it identifier modify grant to a range, which are calculate with this formula 

\hbox{step = INT64\_MAX/n}
$ range = [step * rank, step * (rank + 1)[$ 


[Exemple de requête]
[Exemple de réponse]
\subsubsection{Critical Path}

In progress to find a solution

\subsection{Visualization}

The visualization tool used in the solution is Theia with the Trace Viewer extension. The advantage of this solution is that it implements the TSP protocol. This protocol already defines several routes based on the different views to which the analyzes provide results to display them. Since we have implemented the TSP protocol in the coordinator, we can directly use the Visualization tool without major changes.

Since the TSP protocol does not allow exploring the traces available on the analysis nodes. It was decided that the role fell to the visualization tool to be able to offer the ability to find traces on remote machines. The Trace Viewer viewing tool has a file explorer to navigate folders and files available on the machine wherever the application is running. In the case of the cloud version of Theia the file explorer will be able to explore the files and folders on the remote machine on which the Theia server is started. This brings us to the problem that traces are distributed across different remote machines. Since Theia is not a distributed application capable of exploring files from several machines simultaneously. We decided to offer an alternative solution independent of Theia to be able to open the desired traces for the different analysis nodes. This way other visualization tool that don't implement a file explorer, but only implement the TSP protocol, will be able to use the coordinator.

The user will be able to generate a .trace-coordinator.yml configuration file. The file is separated into two sections. The first is the definition of the analysis nodes available in the cluster and the traces to open when the coordinator is ready. The second part consists of indicating the desired experiments when starting the coordinator. When the file is generated, all that remains is to start the coordinator. When the coordinator is started, traces are opened and experiments created. We can open the visualization Trace Viewer tool and configure the tool so that queries go to the coordinator instead of an analysis node.

\section{Test Environment}

To evaluate our solution, we measured the running time of different analyses on a single Trace Compass server, against our model that includes one coordinator and four Trace Compass servers. Both analyse the same collection of traces. The Trace Compass server instances and the coordinator are deployed on desktop machines located on the same local network. The cluster used in experiments is composed of 22 physical computers, 21 configured as computing nodes and one more as a viewer. Each machine in the cluster contain an Intel Core i5 4570s 2.9Ghz, 16 GB RAM and 1000Go of disk and running Fedora release 38. All traces was generated by tracing the essential kernel events of the machine with Lttng 2.13.9. The collection of traces used for testing is increased in size gradually. For consistency, we make sure to request use the same traces with our solution and with Trace Compass server alone. If analysed by the single Trace Compass server instance, the traces are duplicated by the number of analysis node to simulate the same set of traces. That is, if in the coordinator model each analysis server has 1 GB of traces data, the single instance will handle 1 GB multiply by the number of analysis node.

In the tests conducted to measure the data transferred and the execution time overhead, the client sends requests to the server, for the analysis results from the start to the end of the trace, with realistic values for the resolution. Moreover, the client sends requests for different models. For consistency, we make sure to request the same parameter for each case. Three factors can influence the data transferred and the execution time overhead : the desired resolution of the model, the number of different process, and the size of the trace.

\subsection{Generate Config file}

As we said previously, we need a configuration file to overcome the problems that Theia is not distributed and it will help at the same to indicate where are the analysis node. 

The coordinator config file can be a little long to create if you have more than 100 analysis nodes in your cluster. To simplify the configuration we made a command line interface in Python that you can retrieved it from this repository. \footnote{https://github.com/Spiritus2424/trace-coordinator-cli}

The cli will be able to generate a .trace-coordinator.yml configuration file. You are able to define the IP address and the port for each Trace Compass servers or reuse a host file to list all the IP address and the port in the .trace-coordinator.yml file. At the same time you can indicate the trace path that the trace will be. After you have generate the configuration file you will be able to use the command line to add experiments or to put them manually inside the configuration file.

[Image of .trace-coordinator.yml]

\subsection{Deployment}

To symplify the deployment of the cluster, we use container tool and a configuration management tool.

We used Docker the coordinator image \footnote{https://hub.docker.com/r/spiritus2424/trace-coordinator} and the Trace Compass Server image \footnote{https://hub.docker.com/r/spiritus2424/trace-server}. Each Trace Compass Server runned inside of a container base on alpine tagged 3.16.0 and run on OpenJDK version 17. The coordinator runned inside of a container base on eclipse-temurin tagged 17-alpine and run on OpenJDK version 17. On each physical computer Apptainer \cite{apptainerHome} version 1.1.7-1.fc38 is installed instead of Docker. The reason is that we don't need root access to run a container with Apptainer.

For the configuration management tool, we use Ansible \cite{ansibleAnsibleSimple} version 2.10.8 to automate the configuration and the benchmark.

\section{Results}

The detailed results obtained in this paper and the source code of the experiments are available in this git repository. \footnote{https://github.com/Spiritus2424/trace-coordinator}

\subsection{Trace API with(out) Coordinator}

\subsection{Experiment API with(out) Coordinator}

\subsection{Timegraph API with(out) Coordinator}

\subsection{Xy Graph API with(out) Coordinator}

\subsection{Virtual Table API with(out) Coordinator}

\subsection{Critical Path Analysis with(out) Coordinator}

\section{Discusssion}

\section{Conclusion}

\section{Acknowledgement}