\Chapter{Article 1}\label{sec:architecture}

\section{Introduction}

\section{Related Work}

The related work is divided into six sections. The ﬁrst section will report recent works on Tracing Analysis. The second section will report recent works on integrated development environment. The third section will present different distributed artchitecture that application use for scaling. The fourth section will present recent works on distributed analysis. The fifth section will prensent recent works on dsitributed critical path. The sixth section will present recent works on Theia.


\subsection{Tracing Analysis}

 Tracing involves a specialized use of logging that records runtime events about a program's execution. Tracing is used to detect and identify performance issues, behavior issues and security issues (FIND SOURCE HERE). Tracing can be done on several levels including user-space, kernel, hardware, hypervisor, network, etc. Software that capture those events is called a tracer. To capture those events, tracepoints must be inserted either statically or dynamically. In the case that the tracepoint is inserted statically, the code must be modified to include tracing macros and must be recompiled. In the case that the tracepoint is inserted dynamically, tracepoints are added dynamically to a compiled and running program (Gregg and Mauro, 2011). During the execution of the program, events will be emitted and a tracer will capture them. At this point, a tracer will produce a trace file, organized in a specific format. Because the tracer have to run at the same time of the main application, it will impact the execution time of the main application. This is why a tracer must have a minimal execution overhead. When the tracer finish to produce a trace file, we can apply a trace analysis.

 A trace analysis transforms the trace events into a dataset to perform more investigation and it reorganizes them into data structure for faster access (Prieur-Drevon et al., 2018).

 \subsection{Integrated Development Environment}
 
 % IDE evolution: No plugins, Plugins, Server Protocol, Desktop base IDE, Cloud base IDE, Hybrid base IDE
% Integrated Development Environments (IDEs) have played a pivotal role in shaping the landscape of software development. These powerful software tools have evolved significantly over the years, revolutionizing the way developers write, debug, and maintain code. These IDEs greatly enhanced developer productivity and set the stage for more sophisticated tools. IDEs like Microsoft Visual Studio provided comprehensive solutions for building complex software applications. Developers could now easily navigate through code, leverage libraries, and visually design user interfaces within a single environment. It is clear those IDEs help developers to be more and more productive, depending on the developing phase. The developers get a lot of support from IDEs. The problem is, each time that we add features in IDEs, it become more harder for developer to reach the  productive stage [Article 5]. That is why developers start using IDEs that have basic feature of an text editor and enhance it, using plugins and extension, to become a full featured IDE [Article 6]. 


% Different Server Protocol aujourd'hui mis en place
A lot of work as been put to decouple the support of programming language such as code completion, code error, syntax highlighting and go-to definition. The objective was to enable the support of many programming language using language service [Article 8]. Microsoft's language server protocol (LSP) is a communication protocol, based on JSON-RPC, between a client, which is the IDE, and a server that offers language support. The protocol provides common features such as code completion, code error, syntax highlighting and go-to definition and many more. Many organizations such as the Eclipse Foundation and JetBrains are adapting or creating IDE (Eclipse, IntelliJ) to implement the LSP [Article 7]. 

In the same way, a lot of work as been put to decouple the debugging tools and the IDEs. Microsoft's debugger adapter protocol (DAP) is implement in Visual Studio Code and Theia. The base protocol exchanges messages that consist of a header and a content part (comparable to HTTP). Josselin et al. proposed a protocol that supports meaningful Domain-Specific Languages (DSLs) and that can be used with minimal effort within an IDE that implement DAP. This debugger allowing a generic interactive debugger to communicate with heterogeneous DSL runtime [Article 10]. 

Marr et al. have presented the Kómpos protocol [Article 9] which proposes a concurrency-agnostic debugger protocol that decouples the debugger from the concurrency models employed by the target application. As a result, the underlying language runtime can define custom breakpoints, stepping operations, and execution events for each concurrency model it supports, and a debugger can expose them without having to be specifically adapted. In comparison to existing debugger protocol such as Java Debug Wire Protocol (JDWP) or the GNU Debugger (GDB) machine interface or the Chrome DevTools protocol or Visual Studio Code debug protocol, Kómpos protocol solution is not specific to a concurrency concept.

\subsection{Scaling Distributed Architecture}

Modern Internet services are often implemented as complex, large-scale distributed systems. These applications are constructed from collections of software modules that may be developed by different teams, perhaps in different programming languages, and could span many thousands of machines across multiple physical facilities. Tools that aid in understanding system behavior and reasoning about performance issues are invaluable in such an environment.

Artificial Intelligence (AI) and cloud computing has emerged as a promising avenue for addressing the growing computational demands of AI applications. Parallel and distributed training of AI models become increasingly complex. Parallel and distributed training techniques have emerged as essential approaches to reduce training time and improve resource utilization. By leveraging these techniques, researchers and practitioners can accelerate the training process, enhance model performance, and reduce costs associated with cloud-based AI systems[Article 11].

% Resource demands of HPC applications vary significantly. The varying resource demands can lead to HPC resources being not fully utilized. It becomes very difficult for HPC systems to find the reasons for throttling jobs. [Article 13]

% [Article 14]



\subsection{Distributed Analysis}

% Indeed, trace ﬁles could easily contain millions, even billions of events and the analysis must use an efficient data structure to maintain query performance

Recent efforts to parallelize trace analysis, as exemplified by the work of Reumont-Locke (2015) [17] and Martin (2018) [Article 18], have aimed to enhance the efficiency of this process. Nevertheless, optimizing efficiency alone cannot fully address the scalability challenges inherent in distributed environments. Even if a trace analysis tool operates at peak efficiency, it remains constrained by the computational capabilities of the underlying hardware, typically equivalent to the traced system in the best-case scenario. When we scale up the number of traced nodes to hundreds or thousands, the analysis tool quickly becomes overwhelmed. This bottleneck becomes particularly problematic in applications requiring continuous uptime, as anomalies can rapidly escalate within clusters (Matloff) [20], and delays in data analysis during investigations only exacerbate the situation.

A similar approach observed in distributed tracing platforms involves deploying multiple tracers, each running on a traced machine, with all trace data being transmitted to a central collector for subsequent analysis. In some instances, these tracers take the form of client libraries, allowing for the creation of custom events with application-specific information, in contrast to the more opaque black-box tracing approach of tools like Nagios. Prometheus addresses scalability concerns by enabling the organization of Prometheus instances into a federation, as demonstrated by Reback in 2021 [32]. This approach facilitates the establishment of a multi-level hierarchy of Prometheus servers, with each level aggregating and forwarding data to the level above. Notably, Nagios also adopted a similar approach to mitigate its own scalability challenges (Nagios) [30].

While horizontally scaling analysis servers is an effective strategy, it's essential to recognize that Prometheus analyses primarily consist of simple metrics, making aggregation feasible. However, applying the same model to more complex analysis processes, such as those offered by Trace Compass, presents substantial challenges. The nature of their aggregations varies significantly between different analysis types, making direct application of the Prometheus approach difficult in such cases.

\subsection{Distributed Critical Path}

Recent efforts to parallelize critical path analysis, as exemplified by the work of Denis and Dagenais, have aimed to enhance the efficiency of this process. The kernel events necessary for analysis execution are identified. A distributed architecture that did not require any file transfer between the client and traced nodes, and allowed the distribution of trace analysis computation is presented. They also explore the algorithm to resolve the remote depencies. [Article 19]

Although distributed architectures offer a solution to the scalability problem, it's challenging to fully leverage their benefits due to network communication when broadcasting to all nodes. This is why restricting communications solely to the node essential for the calculation is crucial [Article 20].


\subsection{Theia - Trace Viewer Extension}

Eﬀtinge and Kosyakov have presented Theia (Eﬀtinge and Kosyakov, 2017), a new open-
source IDE framework for building IDEs that could run both as a desktop application or in
a web browser connected to a remote backend. The project shares a lot of similarities with
Microsoft's code editor, Visual Studio Code. Theia uses Node.js, Electron and implements
the LSP.


Theia Trace Viewer extension is the newly developed server-client application developed by Ericsson and the Eclipse Foundation, along with other contributors. It's purpose is to replace Trace Compass as the Eclipse general-purpose tracing tool. As previously described Trace Viewer uses a server-client architecture, where the client communicates with the server through a protocol called Trace Server Protocol (TSP) [10].

\section{Proposed Solution}


\subsection{Motivation \& Background}

Currently, we have several problems related to trace analysis of distributed systems. It is difficult to be able to group all the traces on one machine. If the tracker does not store traces on a machine as Jaeger does, this must be managed by the user. This becomes a problem when we have 1000 and more compute nodes. This adds a workload and sometimes even becomes very limiting. For a small trace of 1GB on each node for 1000 nodes, hard disk space quickly becomes an issue for analyzing distributed system traces.

Trace Compass excels in its ability to support various trace formats, making it a standout framework in this regard. This platform offers a seamless way to incorporate custom front-end parsers and even introduce novel types of analyses. Notably, the separation between the Trace Compass client and server components positions it as a promising option for distributed deployment. While the current Trace Compass server can communicate with a remote client through the Trace Server Protocol, it has limitations. It can only run as a single instance, making it inadequate for handling substantial volumes of trace data in environments like HPC clusters. This limitation becomes apparent when considering the demands on CPU, storage, and RAM memory resources. Additionally, the Trace Compass server presently requires all trace data to be stored locally for analysis, which may not align with certain use cases.

Certainly, running multiple Trace Compass servers, each dedicated to assessing the traces of individual machines within a cluster, is a feasible approach. However, it is crucial to emphasize the importance of having an overarching view of the entire cluster. To make Trace Compass more adept at tracing extensive and diverse systems, a fundamental re-architecture with scalability in mind is imperative. Our aim is to equip Trace Compass with robust query capabilities akin to those found in established platforms, all while preserving its inherent flexibility for the development of novel analysis types and trace formats. Achieving this objective necessitates the implementation of an abstract layer, allowing for the creation of new query operators with minimal effort, while maintaining the overall behavior of the platform. This concept aligns with the notion of the trace parser or the DataProvider layers, which provide an abstraction of the view, irrespective of the specific type of analysis within Trace Compass.


\subsection{Overall architecture}

The architecture platform consists of three main parts, the traced nodes, the analysis nodes and the coordinator node. An analysis node serves as the designated location for processing trace data that originates from traced nodes. This configuration enable us to directly deploy the analysis node where the traces are which is more easy that move the traces at the same node. Each individual analysis node is tasked with handling a distinct set of traced nodes. In our particular configuration, we designate each analysis node as a Trace Compass Server. The coordinator node plays a pivotal role when a client or view initiates a query. It, in turn, dispatches queries to the relevant analysis nodes and subsequently combines their responses to furnish an integrated analysis outcome. When a client transmits a request to a server seeking trace analysis data, the predominant portion of the response typically comprises data points, frequently presented in the form of a time series array.

The need to have a protocol that allows the feasibility or potential to construct complex operations is not necessary in our situation. The reason is because the flexibility of having complex operations is already included in the flexibility of creating analyzes in the Trace Compass architecture. Therefore, we opted for the Trace Server Protocol (TSP) as the communication protocol for both trace data and analysis requests. Several factors guided this decision. Firstly, TSP already possesses the functionality to filter data points based on timestamp parameters, and it can be readily extended to incorporate additional analysis base on current view. Secondly, the Trace Compass server, which we selected for our prototype implementation, already supports this protocol. Thirdly, TSP is designed to break away from existing analytics implementations and focus on displaying data. This allows us to keep the flexibility that Trace Compass has to add or modify analyzes, but also limits the computing power needed on the coordinator, because the coordinator will mainly be aggregating the data together and we want to focus what is compute intensive at analysis node. As a bonus, it help us to easily maintain the coordinator because the aggregation will be base on the views available in the protocol and not the implementation.

[Mettre une figure]

\subsection{}
% \subsection{Connect logically the traces}
% \subsubsection{What are the available traces}
% \subsubsection{Where are the traces}
% \subsubsection{How to connect the traces}

\subsection{Distributed Analysis}

Within the TSP protocol, analysis is restricted to operating on experiments. An experiment is essentially a compilation of all the desired traces required for a particular analysis. Presently, TSP provides four distinct views models for displaying the results of an analysis, depending on the data provider available in the backend: Table, XY Graph, Time Graph, and Data Tree. These models allow the view to directly display data to the user, without prior processing. This makes it possible to avoid resource constraints that certain workstations or applications might have and make the most of the remote resources made available.

In these views, with the exception of virtual table, there are two key axes: the vertical and horizontal axes. The vertical axis represents all the processes within the trace and is visually represent in rows, while the horizontal axis signifies time and is visually displayed in columns.

When an experiment is created and traces are added to it, if the traces are independent, they merely contribute additional rows to the basic analysis, thereby extending the data along the vertical axis. However, if the traces originate from the same tracing session and this session is segmented into multiple trace parts, as exemplified in "Lttng Log Rotation" [Reference], the data can expands both along the vertical and horizontal axes.

In certain scenarios, specific analyses may necessitate access to raw data for completing the analysis, such as the critical path analysis. Regrettably, the TSP does not currently provide any endpoints for retrieving this unprocessed data. Consequently, we have proposed a solution to extend the capabilities of the TSP, enabling users to access data that has not undergone processing for the various views and at the same time allow more feature that could be useful for users in the future.

% In the TSP protocol, we can only run analyzes on $experiments$. An $experiment$ is a list that brings together all the traces desired for an analysis. Currently, TSP allows you to display the results of an analysis on 4 different views depending the data provider available in the backend: Table, XY Graph, Time Graph and Data Tree. In all views, except virtual table lines, we have the vertical axis and the horizontal axis. The vertical axis is all the process that are in the trace and it graphically represent by rows. The horizontal axis is the time and it graphically represent by column.

% When we create an experiment and we add traces inside. If the trace are independent, it will only add rows to basic analysis. So the data will extend on the  vertical axis. If the trace are from the same tracing session and we split this session into many parts of trace, like Lttng Log Rotation [Article ?]. At this time the data will extend on the vertical axis and the horizontal axis. 

% In some case, some analysis will have to use some raw data to complete analysis, like the critical path. At this point, the TSP do not offer endpoint to access the raw data. So we proposed an solution to extends the TSP and allow the user to retrieve data that is not processed for the views.

\subsubsection{Aggregation of independent trace (vertically)}

TSP conveniently exposes the analysis results in the form of a view model data. As previously discussed, this approach offers the advantage of enabling the client to directly display the data without the need for additional processing. Consequently, our proposed solution focuses its aggregation efforts on this preformatted result, as opposed to the raw data retrieved when querying the state system, as is the case with Trace Compass.

Most of the analyses are currently represented as a series of data points over a timeline. Therefore, when requesting an analysis result via TSP, it primarily necessitates providing the timestamps for the analyzed timeline and the requested items for the analyzed process that we want to show [Example query]. Upon receiving requests from the client or view, the coordinator node can efficiently retain the request in its original form, duplicate it, and broadcast it to the other nodes. Subsequently, it aggregates the returned responses in a cohesive manner.

This approach works well because we are analyzing traces from different machines, generated independently but during the same time frame. As a result, the querying time interval remains consistent. In essence, we can liken this process to a Map/Reduce paradigm, with the distributed data processing involving the analysis nodes acting as the workers during the Map phases and the coordinator node overseeing the Reduce phases. Upon receiving a request, the coordinator queries the analysis nodes and then merges all their responses, to produce the final response for the client. 

Let us take a use case that, when aggregating data from multiple sources, computes the ratio of each value to the "sum" of data (e.g. percentage of each CPU usage). Because the trace are independent and make a percentage base on the resources of the machine that the trace was collected, we don't need to recalculate the ratio of each value. 

The problem is that each analysis nodes are independent and don't have the notion of what other nodes could produce. Each rows that is display in the view are suppose to have an unique identifier and not an universal unique identifier. So each node analysis when they are generating the data. It is incrementing a counter for each rows that is create that start from 0. Therefore, when the coordinator will aggregates the response from each analysis nodes, it will send back to the client/views many series of data points with the same identifier. To solve this problem, the coordinator will take responsibility to change the identifier of each 



[Exemple de requête]
[Exemple de réponse]
\subsubsection{Critical Path}

\subsection{Visualization}

% Talk about TSP reuse of the view and all the view that implemented the TSP protocol

\section{Test Environment}

\subsection{Computer Specification}

\subsection{Generate Config file}

\subsection{Deployment}

\section{Results}

\subsection{Trace API with(out) Coordinator}

\subsection{Experiment API with(out) Coordiantor}

\subsection{Timegraph API with(out) Coordiantor}

\subsection{Xy Graph API with(out) Coordiantor}

\subsection{Virtual Table API with(out) Coordiantor}

\subsection{Critical Path Analysis with(out) Coordiantor}

\section{Discusssion}

\section{Conclusion}

\section{Acknowledgement}